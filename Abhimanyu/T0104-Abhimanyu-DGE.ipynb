{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from UtilityLib import ProjectManager\n",
    "\n",
    "class TCGAProject(ProjectManager):\n",
    "\n",
    "  def __init__(self, *args, **kwargs):\n",
    "    super().__init__(*args, **kwargs)\n",
    "\n",
    "  # Overridden Methods START\n",
    "  def json_to_df(self, *args, **kwargs):\n",
    "\n",
    "    \"\"\"JSON structure to DataFrame converter\n",
    "\n",
    "    0|json: JSON file path (string)/object (dict)\n",
    "    1|map: Dot notation of keys to parse (parsable using UL.deepkey) i.e., column to key map e.g. entity|0|metadata|header\n",
    "\n",
    "    @return\n",
    "    Pandas DataFrame object\n",
    "\n",
    "    \"\"\"\n",
    "    _json = args[0] if len(args) > 0 else kwargs.get(\"json\")\n",
    "    _map = args[1] if len(args) > 1 else kwargs.get(\"map\", None)\n",
    "    _sep = args[2] if len(args) > 2 else kwargs.get(\"sep\", '.')\n",
    "\n",
    "    if not hasattr(kwargs, 'sep'):\n",
    "      kwargs.update({\"sep\": _sep})\n",
    "\n",
    "    if isinstance(_json, (str)):\n",
    "      _json = self.read_json(_json)\n",
    "\n",
    "    _result = []\n",
    "\n",
    "    for _json_el in _json:\n",
    "      _row = {}\n",
    "      if _map and isinstance(_map, (dict)):\n",
    "        # If column map is provided\n",
    "        for _column, _dotkey in _map.items():\n",
    "          _row[_column] = self.get_deep_key(_json_el, _dotkey, **kwargs)\n",
    "      else:\n",
    "        # If column map is not provided\n",
    "        for _column, _value in _json.items():\n",
    "          _row[_column] = _value\n",
    "      _result.append(_row)\n",
    "\n",
    "    return self.DF(_result)\n",
    "\n",
    "  def get_deep_key(self, *args, **kwargs):\n",
    "    _obj = args[0] if len(args) > 0 else kwargs.get(\"obj\", {})\n",
    "    _keys = args[1] if len(args) > 1 else kwargs.get(\"keys\", ())\n",
    "    _default = args[2] if len(args) > 2 else kwargs.get(\"default\")\n",
    "    _sep = args[3] if len(args) > 3 else kwargs.get(\"sep\", \"|\")\n",
    "\n",
    "    _instance_list = (tuple, set, list)\n",
    "    _instance_dict = (dict)\n",
    "    _instance_singluar = (str, int)\n",
    "\n",
    "    _keys = _keys if isinstance(_keys, _instance_list) else _keys.split(_sep)\n",
    "\n",
    "    for _k in _keys:\n",
    "      # hasattr(, 'get') & str|int=> _dict key, int => list, tuple, or set\n",
    "      if \"*\" in _k:\n",
    "        _obj = list(_obj)\n",
    "      elif isinstance(_obj, _instance_dict) and isinstance(_k, _instance_singluar):\n",
    "        _obj = _obj.get(_k, _default)\n",
    "      elif(isinstance(_obj, _instance_list) and (isinstance(_k, _instance_singluar) or _k.isnumeric())):\n",
    "        _k = int(_k)\n",
    "        if len(_obj) > _k:\n",
    "          _obj = _obj[_k]\n",
    "\n",
    "    return _obj\n",
    "\n",
    "  def fix_column_names(self, *args, **kwargs):\n",
    "    _df = args[0] if len(args) > 0 else kwargs.get(\"df\")\n",
    "    _df.columns = [self.text_to_slug(_col).replace('-', '_') for _col in _df.columns]\n",
    "  # Overridden Methods END\n",
    "\n",
    "  _gdc_manifest = None\n",
    "  def parse_gdc_manifest(self):\n",
    "    __gdm = self.pd_tsv(self.find_files(self.get_path(), \"gdc_manifest*\")[0])\n",
    "    __gdm.columns = [_col.replace(' ', '_') for _col in __gdm.columns]\n",
    "    self.pd_excel(self.get_path(\"gdc-manifest.xlsx\"), __gdm, \"Manifest\")\n",
    "    self._gdc_manifest = __gdm\n",
    "\n",
    "  def _convert_file_set_to_df(self, _file_set, _chunk_path):\n",
    "    _expression_df = None\n",
    "    for _f in _file_set:\n",
    "      _fn = self.filename(_f, with_ext=True)\n",
    "      if _fn in self.config.exp.star_files:\n",
    "        continue\n",
    "      _df = self.pd_tsv(_f, skiprows=1)\n",
    "      _df['FileName'] = _fn\n",
    "      _expression_df = _df if _expression_df is None else self.PD.concat([_expression_df, _df], axis=0)\n",
    "      self.config.exp.star_files.append(_fn)\n",
    "    _expression_df.to_pickle(_chunk_path)\n",
    "\n",
    "  file_expression = 'expression-star-count.pkl.gz'\n",
    "  _expression_df = None\n",
    "  def get_expression_data(self):\n",
    "    _exp_path = self.get_path(self.file_expression)\n",
    "    if self.exists(_exp_path):\n",
    "      self._expression_df = self.unpickle(_exp_path)\n",
    "    else:\n",
    "      self.process_expression_data()\n",
    "\n",
    "  def get_keymap(self, _type):\n",
    "    return self.config.key_maps.get(_type)\n",
    "\n",
    "  def set_json_parse_key_maps(self):\n",
    "    self.config.key_maps.clinical = {\n",
    "        'alcohol_history': 'exposures|0|alcohol_history',\n",
    "        'case_id': 'case_id',\n",
    "        'project_id': 'project|project_id',\n",
    "        'submitter_id': 'submitter_id',\n",
    "        'synchronous_malignancy': 'diagnoses|0|synchronous_malignancy',\n",
    "        'ajcc_pathologic_stage': 'diagnoses|0|ajcc_pathologic_stage',\n",
    "        'days_to_diagnosis': 'diagnoses|0|days_to_diagnosis',\n",
    "        'treatment_type': 'diagnoses|0|treatments|0|treatment_type',\n",
    "        'treatment_or_therapy': 'diagnoses|0|treatments|0|treatment_or_therapy',\n",
    "        'tissue_or_organ_of_origin': 'diagnoses|0|tissue_or_organ_of_origin',\n",
    "        'primary_diagnosis': 'diagnoses|0|primary_diagnosis',\n",
    "        'age_at_diagnosis': 'diagnoses|0|age_at_diagnosis',\n",
    "        'ajcc_pathologic_t': 'diagnoses|0|ajcc_pathologic_t',\n",
    "        'ajcc_pathologic_n': 'diagnoses|0|ajcc_pathologic_n',\n",
    "        'ajcc_pathologic_m': 'diagnoses|0|ajcc_pathologic_m',\n",
    "        'classification_of_tumor': 'diagnoses|0|classification_of_tumor',\n",
    "        'site_of_resection_or_biopsy': 'diagnoses|0|site_of_resection_or_biopsy',\n",
    "        'tumor_grade': 'diagnoses|0|tumor_grade',\n",
    "        'icd_10_code': 'diagnoses|0|icd_10_code',\n",
    "        'days_to_birth': 'demographic|days_to_birth',\n",
    "        'vital_status': 'demographic|vital_status',\n",
    "        'race': 'demographic|race',\n",
    "        'gender': 'demographic|gender',\n",
    "        'ethnicity': 'demographic|ethnicity',\n",
    "    }\n",
    "\n",
    "    self.config.key_maps.biospecimen = {\n",
    "        'case_id': 'case_id',\n",
    "        'project_id': 'project|project_id',\n",
    "        'submitter_id': 'submitter_id',\n",
    "        'samples': 'samples'\n",
    "      }\n",
    "\n",
    "    self.config.key_maps.metadata = {\n",
    "        \"submitter_id\": \"submitter_id\",\n",
    "        # \"data_category\": \"data_category\",\n",
    "        \"data_format\": \"data_format\",\n",
    "        \"associated_case_id\": \"associated_entities.0.case_id\",\n",
    "        # \"file_name\": \"file_name\",\n",
    "        \"file_id\": \"file_id\",\n",
    "        # \"data_type\": \"data_type\",\n",
    "        \"experimental_strategy\": \"experimental_strategy\",\n",
    "      }\n",
    "\n",
    "  _dir_chunks = 'Read-Chunks'\n",
    "  _size_chunks = 10\n",
    "  test_chunks = None\n",
    "  _total_chunks = None\n",
    "  dir_tcga_downloads = \"TCGA-Downloads\"\n",
    "  template_file_chunk = \"File-Chunk-%s.pkl.gz\"\n",
    "\n",
    "  def _process_chunks(self):\n",
    "    if not isinstance(self.config.exp.star_files, (list, tuple)):\n",
    "      self.config.exp.star_files = []\n",
    "\n",
    "    self.validate_dir(self.get_path(self._dir_chunks))\n",
    "    self.init_multiprocessing()\n",
    "\n",
    "    _expression_files = self.find_files(self.get_path(self.dir_tcga_downloads), \"*/*tsv\")\n",
    "    _chunk_list = list(self.chunks(_expression_files, self._size_chunks))\n",
    "    _chunk_list = _chunk_list[:self.test_chunks]\n",
    "    self._total_chunks = len(_chunk_list)\n",
    "\n",
    "    for _chunk_idx, _file_set in self.PB(enumerate(_chunk_list), total=self._total_chunks):\n",
    "      _chunk_path = self.get_path(f'{self._dir_chunks}/{self.template_file_chunk}' % _chunk_idx)\n",
    "      if self.exists(_chunk_path):\n",
    "        continue\n",
    "\n",
    "      if False:\n",
    "        self._convert_file_set_to_df(_file_set, _chunk_path)\n",
    "      else:\n",
    "        self.queue_task(self._convert_file_set_to_df, _file_set, _chunk_path)\n",
    "\n",
    "    self.process_queue()\n",
    "\n",
    "  _combined_df = None\n",
    "  def combine_chunks(self):\n",
    "    \"\"\"Combined all the read files\n",
    "    This isn't working as dimension of the data is getting very huge\n",
    "    \"\"\"\n",
    "    _pkl_dfs = self.find_files(self.get_path(self._dir_chunks), \"*pkl.gz\")\n",
    "    _pkl_dfs_reads = []\n",
    "    for _pkf in self.PB(_pkl_dfs):\n",
    "        _pkl_dfs_reads.append(self.unpickle(_pkf))\n",
    "\n",
    "    self._combined_df = self.PD.concat(_pkl_dfs_reads, axis=0)\n",
    "\n",
    "  def get_chunk_files(self, ext='pkl.gz'):\n",
    "    return self.find_files(self.get_path(self._dir_chunks), ext)\n",
    "\n",
    "  sample_type_codes_map = None\n",
    "  def set_gdc_sample_type_codes(self):\n",
    "    self.config.path_sample_codes = self.get_path(\"sample_type_codes.pkl.gz\")\n",
    "    if not self.exists(self.config.path_sample_codes):\n",
    "      _tables = self.PD.read_html(\"https://gdc.cancer.gov/resources-tcga-users/tcga-code-tables/sample-type-codes\")\n",
    "      __sc_df = _tables[-1]\n",
    "      self.fix_column_names(__sc_df)\n",
    "      __sc_df.Code = __sc_df.Code.apply(lambda x: f\"{x:02d}\")\n",
    "      __sc_df = dict(zip(__sc_df.Code, __sc_df.Definition))\n",
    "      self.pickle(self.config.path_sample_codes, __sc_df)\n",
    "\n",
    "    self.sample_type_codes_map = self.unpickle(self.config.path_sample_codes)\n",
    "\n",
    "  def sample_code_def(self, _code):\n",
    "    return self.sample_type_codes_map.get(_code)\n",
    "\n",
    "  def get_chunk_status(self):\n",
    "    _qs = self.queue_task_status()\n",
    "    _qs['open_files'] = self.get_open_file_descriptors()\n",
    "    _qs['processed_chunks'] = len(self.get_chunk_files())\n",
    "    _qs['total_chunks'] = self._total_chunks\n",
    "    self.update_config()\n",
    "    return _qs\n",
    "\n",
    "  def process_expression_data(self):\n",
    "    self._process_chunks()\n",
    "    # Wait while all chunks are process using chunk count etc\n",
    "\n",
    "  def parse_sample_details(self, _sample):\n",
    "    \"\"\"Parse Sample Details\"\"\"\n",
    "    _sid = _sample.get('sample_type_id', \"99\")\n",
    "    return {\n",
    "        \"sample_type_id\": _sid,\n",
    "        \"sample_type_alt\": self.sample_code_def(_sid),\n",
    "        \"sample_type\": _sample.get('sample_type'),\n",
    "        \"tumor_descriptor\": _sample.get('tumor_descriptor'),\n",
    "        \"sample_id\": _sample.get('sample_id'),\n",
    "        \"specimen_type\": _sample.get('specimen_type'),\n",
    "        \"is_ffpe\": _sample.get('is_ffpe'),\n",
    "        \"preservation_method\": _sample.get('preservation_method'),\n",
    "        \"tissue_type\": _sample.get('tissue_type'),\n",
    "      }\n",
    "\n",
    "T0104 = TCGAProject(path_bases=[r\"/mnt/DataDrive/MDD/T0104--Abhimanyu-GeneExpression/TCGA-BrCa-Abhimanyu-20240514\"], test_chunks = 5)\n",
    "\n",
    "\"\"\"Data Download\"\"\"\n",
    "# T0104.process_expression_data()\n",
    "# T0104.update_config()\n",
    "\n",
    "\"\"\"Data Parsing\"\"\"\n",
    "T0104.set_gdc_sample_type_codes()\n",
    "T0104.set_json_parse_key_maps()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GDC Client Downloaded Files\n",
    "_gdc_manifest = T0104.pd_tsv(T0104.get_path('gdc_manifest.2024-05-14.txt'))\n",
    "T0104.fix_column_names(_gdc_manifest)\n",
    "print(_gdc_manifest.shape)\n",
    "_gdc_manifest.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Sample Sheet\n",
    "\n",
    "_sample_sheet = T0104.pd_tsv(T0104.get_path(\"gdc_sample_sheet.2024-05-14.tsv\"))\n",
    "T0104.fix_column_names(_sample_sheet)\n",
    "print(_sample_sheet.shape)\n",
    "_sample_sheet.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_metadata = T0104.json_to_df(T0104.get_path(\"metadata.cohort.2024-05-14.json\"), T0104.get_keymap('metadata'))\n",
    "print(_metadata.shape)\n",
    "_metadata.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_merged_meta_sampl = T0104.PD.merge(_sample_sheet, _metadata, left_on='File_ID', right_on='file_id')\n",
    "_merged_meta_sampl.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_clinical = T0104.json_to_df(T0104.get_path(\"clinical.cohort.2024-05-14--COHORT.json\"), T0104.get_keymap('clinical'), sep='|')\n",
    "print(_clinical.shape)\n",
    "_clinical.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_biospecimen = T0104.json_to_df(T0104.get_path(\"biospecimen.cohort.2024-05-14--COHORT.json\"), T0104.get_keymap('biospecimen'), sep='|')\n",
    "print(_biospecimen.shape)\n",
    "_biospecimen.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_merged_cli_bspeci = T0104.PD.merge(_biospecimen[['case_id', 'samples']], _clinical, on='case_id')\n",
    "print(_merged_cli_bspeci.shape)\n",
    "_merged_cli_bspeci.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exapnd BioSpecimen Samples\n",
    "\n",
    "_all_samples = []\n",
    "\n",
    "for _sidx, _sample_det in _merged_cli_bspeci.iterrows():\n",
    "  _sdict = _sample_det.to_dict()\n",
    "  _slist = _sdict.pop('samples')\n",
    "  for _sample in _slist:\n",
    "    _sdata = T0104.parse_sample_details(_sample)\n",
    "    _sdata.update(_sdict)\n",
    "    _all_samples.append(_sdata)\n",
    "  #   break\n",
    "  # break\n",
    "\n",
    "_all_samples_df = T0104.DF(_all_samples)\n",
    "print(_all_samples_df.shape)\n",
    "_all_samples_df.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merged Metadata Mapping as Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_annotated_ds = T0104.PD.merge(_all_samples_df, _merged_meta_sampl, left_on='case_id', right_on='associated_case_id')\n",
    "\n",
    "T0104.config.path_sample_excel = T0104.get_path(\"T0104--sample-annotations.xlsx\")\n",
    "\n",
    "T0104.pd_excel(T0104.config.path_sample_excel, _annotated_ds, 'Annotated-Samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(_all_samples_df.columns)\n",
    "_all_samples_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(_merged_meta_sampl.columns)\n",
    "_merged_meta_sampl.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_assoc_cid = set(_merged_meta_sampl.associated_case_id)\n",
    "_smpl_cid = set(_all_samples_df.case_id)\n",
    "\n",
    "len(_assoc_cid), len(_smpl_cid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(_assoc_cid | _smpl_cid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_filter_smpl_typ =_annotated_ds.sample_type.isin(['Primary Tumor', 'Solid Tissue Normal', 'Metastatic'])\n",
    "_filter_spcmn_typ =_annotated_ds.sample_type.isin(['Solid Tissue'])\n",
    "_filter_ajcc_pthl_stg = _annotated_ds.ajcc_pathologic_stage.isin(['Stage IA', 'Stage IIB', 'Stage IIA', 'Stage IIIA', 'Stage IV',\n",
    "       'Stage IIIC', 'Stage I', 'Stage IIIB', 'Stage IB',\n",
    "       'Stage II', 'Stage X', 'Stage III'])\n",
    "_ad_f1 = _annotated_ds[_filter_smpl_typ & _filter_ajcc_pthl_stg].copy()\n",
    "\n",
    "# T0104.pd_excel(T0104.config.path_sample_excel, _ad_f1, 'Filtered-Samples')\n",
    "\n",
    "print(_annotated_ds.shape, _ad_f1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_samples_Normal = _ad_f1[_ad_f1.tissue_type.eq('Normal')].copy()\n",
    "_df_samples_Tumor = _ad_f1[_ad_f1.tissue_type.eq('Tumor')].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_samples_Normal.shape, _df_samples_Tumor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ad_f1.tissue_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_uvals = []\n",
    "for _col in _ad_f1.columns:\n",
    "    _uvals.append((_col, _ad_f1[_col].nunique()))\n",
    "\n",
    "_uvals_df = T0104.DF(_uvals, columns=['Column', 'Unique_Values'])\n",
    "_uvals_df[_uvals_df.Unique_Values.between(2, 1290)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tumor vs Normal Expression Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_list_files_Normal = _df_samples_Normal.File_Name.to_list()\n",
    "_list_files_Tumor = _df_samples_Tumor.File_Name.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to merge statistics of gene expression\n",
    "\n",
    "T0104.require('numpy', 'NP')\n",
    "\n",
    "def merge_gxp_stats(_exp1, _exp2):\n",
    "  _key_gid = 'gene_id'\n",
    "  if _exp1.get(_key_gid) == _exp2.get(_key_gid):\n",
    "    _key_count = 'FPKM_count'\n",
    "    _key_min = 'FPKM_min'\n",
    "    _key_mean = 'FPKM_mean'\n",
    "    _key_max = 'FPKM_max'\n",
    "    _key_std = 'FPKM_std'\n",
    "    _key_median = 'FPKM_median'\n",
    "    # Combine counts\n",
    "    _final_count = _exp1[_key_count] + _exp2[_key_count]\n",
    "\n",
    "    # Calculate combined mean\n",
    "    _final_mean = (_exp1[_key_mean] * _exp1[_key_count] + _exp2[_key_mean] * _exp2[_key_count]) / _final_count\n",
    "\n",
    "    # Calculate combined standard deviation\n",
    "    _final_variance = (\n",
    "        ((_exp1[_key_count] - 1) * _exp1[_key_std]**2 + (_exp2[_key_count] - 1) * _exp2[_key_std]**2) +\n",
    "        (_exp1[_key_count] * (_exp1[_key_mean] - _final_mean)**2 + _exp2[_key_count] * (_exp2[_key_mean] - _final_mean)**2)\n",
    "    ) / (_final_count - 1)\n",
    "    _final_std = T0104.NP.sqrt(_final_variance)\n",
    "\n",
    "    # Calculate combined min and max\n",
    "    _final_min = min(_exp1[_key_min], _exp2[_key_min])\n",
    "    _final_max = max(_exp1[_key_max], _exp2[_key_max])\n",
    "\n",
    "    # Calculate combined median\n",
    "    _final_values = [_exp1[_key_median]] * _exp1[_key_count] + [_exp2[_key_median]] * _exp2[_key_count]\n",
    "    _final_median = T0104.NP.median(_final_values)\n",
    "\n",
    "    # Create the combined dictionary\n",
    "    _final_dict = {\n",
    "      _key_gid: _exp1.get(_key_gid),\n",
    "      _key_min: _final_min,\n",
    "      _key_max: _final_max,\n",
    "      _key_std: _final_std,\n",
    "      _key_count: _final_count,\n",
    "      _key_mean: _final_mean,\n",
    "      _key_median: _final_median\n",
    "    }\n",
    "\n",
    "    return _final_dict\n",
    "  else:\n",
    "    return _exp1\n",
    "\n",
    "def store_gene_exp(dtyp, _gdata):\n",
    "  _gene_id = _gdata.gene_id\n",
    "  _gexp = _gdata.to_dict()\n",
    "  if not _gene_id in T0104.config.expt3[dtyp]:\n",
    "    T0104.config.expt3[dtyp][_gene_id] = _gexp\n",
    "  else:\n",
    "    _stats = merge_gxp_stats(T0104.config.expt3[dtyp][_gene_id], _gexp)\n",
    "    T0104.config.expt3[dtyp][_gene_id] = _stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_chunks = T0104.get_chunk_files()\n",
    "\n",
    "for _c in T0104.PB(_chunks):\n",
    "  _cdf = T0104.unpickle(_c)\n",
    "  _cdf = _cdf[~_cdf.gene_id.str.startswith('N_')]\n",
    "  _cdf_Tumor = _cdf[_cdf.FileName.isin(_list_files_Tumor)]\n",
    "  _cdf_Normal = _cdf[_cdf.FileName.isin(_list_files_Normal)]\n",
    "\n",
    "  _cdf_Tumor_summ =  _cdf_Tumor.groupby('gene_id').agg(\n",
    "      # Files = ('FileName', list),\n",
    "      FPKM_min = ('fpkm_uq_unstranded', 'min'),\n",
    "      FPKM_max = ('fpkm_uq_unstranded', 'max'),\n",
    "      FPKM_std = ('fpkm_uq_unstranded', 'std'),\n",
    "      FPKM_count = ('fpkm_uq_unstranded', 'count'),\n",
    "      FPKM_mean = ('fpkm_uq_unstranded', 'mean'),\n",
    "      FPKM_median = ('fpkm_uq_unstranded', 'median'),\n",
    "  ).reset_index()\n",
    "\n",
    "  _cdf_Normal_summ =  _cdf_Normal.groupby('gene_id').agg(\n",
    "      # Files = ('FileName', list),\n",
    "      FPKM_min = ('fpkm_uq_unstranded', 'min'),\n",
    "      FPKM_max = ('fpkm_uq_unstranded', 'max'),\n",
    "      FPKM_std = ('fpkm_uq_unstranded', 'std'),\n",
    "      FPKM_count = ('fpkm_uq_unstranded', 'count'),\n",
    "      FPKM_mean = ('fpkm_uq_unstranded', 'mean'),\n",
    "      FPKM_median = ('fpkm_uq_unstranded', 'median'),\n",
    "  ).reset_index()\n",
    "\n",
    "  for _ix, _ir in _cdf_Tumor_summ.iterrows():\n",
    "    store_gene_exp('Tumor', _ir)\n",
    "\n",
    "  for _ix, _ir in _cdf_Normal_summ.iterrows():\n",
    "    store_gene_exp('Normal', _ir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_Tumor_exp = T0104.DF(T0104.config.expt3.Tumor.values())\n",
    "_Normal_exp = T0104.DF(T0104.config.expt3.Normal.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_compare_ids = _Tumor_exp.head(5).gene_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_Normal_exp[_Normal_exp.gene_id.isin(_compare_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_Tumor_exp[_Tumor_exp.gene_id.isin(_compare_ids)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
